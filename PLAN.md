##  PLAN DU PROJET ‚Äì √âvaluation M√©thodologique des Mod√®les de Langage Locaux (LLMs)

###  Objectif g√©n√©ral
Mettre en place une **m√©thodologie rigoureuse et automatis√©e** d‚Äô√©valuation des mod√®les de langage (LLMs) locaux, adapt√©e √† des cas d‚Äôusage de type **RAG (Retrieval-Augmented Generation)**.

Le projet repose sur la cr√©ation d‚Äôun **pipeline complet** combinant :
- des jeux de prompts et de jeux de donn√©es (JDD),
- l‚Äôex√©cution automatique sur diff√©rents mod√®les locaux,
- l‚Äô√©valuation par des m√©triques objectives et personnalisables,
- l‚Äôanalyse comparative des performances,
- l‚Äôint√©gration d‚Äôun syst√®me de monitoring ou d‚Äôaudit pour l‚Äôobservabilit√© des mod√®les.

---

###  Objectifs techniques
- D√©finir un **ensemble de m√©triques** pour l‚Äô√©valuation des r√©ponses g√©n√©r√©es
- D√©velopper un **pipeline automatis√©** d'ex√©cution de tests (via Python)
- Permettre l‚Äô√©valuation de diff√©rents mod√®les ex√©cut√©s **en local** (Ollama, LM Studio, etc.)
- Centraliser les r√©sultats dans un format exploitable (CSV / JSON)
- Fournir des **analyses visuelles et statistiques** des r√©sultats
- Concevoir un d√©but de **syst√®me de monitoring** pour d√©tecter les d√©rives ou comportements anormaux

---

##  Organisation du travail ‚Äì 2 Semaines

###  Semaine 1 : Conception et Mise en place

| Jour | √âtape | R√©sultat attendu |
|------|-------|------------------|
| J1   | Cadrage du projet | Objectifs + p√©rim√®tre technique clair |
| J2   | Choix des m√©triques | Grille d‚Äô√©valuation des mod√®les |
| J3   | Conception des prompts et JDD | Fichier `rag_queries.json` pr√™t √† l‚Äôemploi |
| J4   | Installation des mod√®les locaux | Ollama pr√™t avec Mistral, LLaMA3, Phi, Qwen, etc. |
| J5   | D√©veloppement du script de test | Pipeline Python initial (input ‚Üí output ‚Üí logs) |

###  Semaine 2 : Tests, Analyse et Valorisation

| Jour | √âtape | R√©sultat attendu |
|------|-------|------------------|
| J6   | Lancement des tests | R√©sultats bruts dans `results/` |
| J7   | Analyse comparative | Graphiques, matrices de score |
| J8   | Recommandations | Classement des mod√®les selon les usages |
| J9   | Syst√®me de monitoring | D√©but de syst√®me d‚Äôalerte ou suivi |
| J10  | Rapport final | Rapport + pr√©sentation pr√™t √† livrer |

---

##  √âtapes du pipeline LLM √† impl√©menter

1. **Pr√©paration des prompts et du jeu de donn√©es**
2. **Ex√©cution automatique** : chaque mod√®le re√ßoit chaque prompt
3. **Mesure des performances** : temps, RAM, verbosit√©, etc.
4. **√âvaluation de la qualit√©** : notation, scoring contextuel, etc.
5. **Centralisation des r√©sultats** dans un format standard
6. **Analyse comparative** : visualisation, stats, classements
7. **Monitoring / Observabilit√©** (√©tape avanc√©e)

---

## üìä M√©triques envisag√©es

| Crit√®re | Description |
|--------|-------------|
|  Pertinence de la r√©ponse | Qualit√© et exactitude du contenu g√©n√©r√© |
|  Utilisation correcte du contexte | Bon usage du corpus de r√©f√©rence |
|  Temps de g√©n√©ration | D√©lai entre question et r√©ponse |
|  M√©moire utilis√©e | RAM/CPU/GPU mobilis√©s |
|  Verbosit√© | Longueur, r√©p√©tition inutile |
|  Robustesse | R√©action face aux erreurs ou donn√©es bruit√©es |
|  Monitoring | Suivi des comportements anormaux |

---

##  Livrables attendus
- Scripts Python de test et d‚Äôanalyse
- Jeu de prompts complet (`prompts/rag_queries.json`)
- R√©sultats export√©s (`results/model_outputs.csv`)
- Visualisations d‚Äôanalyse (`analysis/`)
- Rapport final structur√© (`report/rapport_final.md` ou PDF)
- README document√© et clair

---

##  Prochaines √©tapes
- [ ] Ajouter le jeu de prompts
- [ ] Compl√©ter les scripts
- [ ] Int√©grer les mod√®les
- [ ] Ex√©cuter les tests
- [ ] Lancer les analyses
- [ ] Documenter et livrer

---

